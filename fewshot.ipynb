{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alphabet: Alphabet_of_the_Magi\n",
      "loading alphabet: Anglo-Saxon_Futhorc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knnar\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\ipykernel\\__main__.py:27: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alphabet: Arcadian\n",
      "loading alphabet: Armenian\n",
      "loading alphabet: Asomtavruli_(Georgian)\n",
      "loading alphabet: Balinese\n",
      "loading alphabet: Bengali\n",
      "loading alphabet: Blackfoot_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Braille\n",
      "loading alphabet: Burmese_(Myanmar)\n",
      "loading alphabet: Cyrillic\n",
      "loading alphabet: Early_Aramaic\n",
      "loading alphabet: Futurama\n",
      "loading alphabet: Grantha\n",
      "loading alphabet: Greek\n",
      "loading alphabet: Gujarati\n",
      "loading alphabet: Hebrew\n",
      "loading alphabet: Inuktitut_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Japanese_(hiragana)\n",
      "loading alphabet: Japanese_(katakana)\n",
      "loading alphabet: Korean\n",
      "loading alphabet: Latin\n",
      "loading alphabet: Malay_(Jawi_-_Arabic)\n",
      "loading alphabet: Mkhedruli_(Georgian)\n",
      "loading alphabet: N_Ko\n",
      "loading alphabet: Ojibwe_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Sanskrit\n",
      "loading alphabet: Syriac_(Estrangelo)\n",
      "loading alphabet: Tagalog\n",
      "loading alphabet: Tifinagh\n",
      "(964, 20, 105, 105)\n",
      "loading alphabet: Angelic\n",
      "loading alphabet: Atemayar_Qelisayer\n",
      "loading alphabet: Atlantean\n",
      "loading alphabet: Aurek-Besh\n",
      "loading alphabet: Avesta\n",
      "loading alphabet: Ge_ez\n",
      "loading alphabet: Glagolitic\n",
      "loading alphabet: Gurmukhi\n",
      "loading alphabet: Kannada\n",
      "loading alphabet: Keble\n",
      "loading alphabet: Malayalam\n",
      "loading alphabet: Manipuri\n",
      "loading alphabet: Mongolian\n",
      "loading alphabet: Old_Church_Slavonic_(Cyrillic)\n",
      "loading alphabet: Oriya\n",
      "loading alphabet: Sylheti\n",
      "loading alphabet: Syriac_(Serto)\n",
      "loading alphabet: Tengwar\n",
      "loading alphabet: Tibetan\n",
      "loading alphabet: ULOG\n",
      "(659, 20, 105, 105)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "\n",
    "path = ''\n",
    "train_path = os.path.join(path, 'images_background')\n",
    "valid_path = os.path.join(path, 'images_evaluation')\n",
    "\n",
    "def omniglot_loader(path):\n",
    "    \n",
    "    img_array = []\n",
    "    \n",
    "    for alphabet in os.listdir(path):\n",
    "        print(\"loading alphabet: \" + alphabet)\n",
    "        alphabet_path = os.path.join(path, alphabet)\n",
    "        \n",
    "        for letter in os.listdir(alphabet_path):\n",
    "            alphabet_images = []\n",
    "            letter_path = os.path.join(alphabet_path, letter)\n",
    "            \n",
    "            if not os.path.isdir(letter_path):\n",
    "                continue\n",
    "            \n",
    "            for image in os.listdir(letter_path):\n",
    "                image_path = os.path.join(letter_path, image)\n",
    "                image = imread(image_path)\n",
    "                \n",
    "                image = image / 255\n",
    "                image = 1 - image\n",
    "                \n",
    "                alphabet_images.append(image)\n",
    "                \n",
    "            try:\n",
    "                img_array.append(np.stack(alphabet_images))\n",
    "                \n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"error - alphabet_images\", alphabet_images)\n",
    "    \n",
    "    img_array = np.stack(img_array)\n",
    "    return img_array\n",
    "\n",
    "xTrain = omniglot_loader(train_path)\n",
    "print(xTrain.shape)\n",
    "\n",
    "xValid = omniglot_loader(valid_path)\n",
    "print(xValid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Creation\n",
    "\n",
    "The following cells create the batches for the training and validation sets. create_val_batch accepts arguments for how many different images the validation image will be tested against. The higher the value of N, the more difficult the few-shot task will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_batch(dataset, batch_size):\n",
    "    \n",
    "    classes, examples, w, h = dataset.shape\n",
    "    \n",
    "    pairs = [np.zeros((batch_size, h, w, 1)) for i in range(2)]\n",
    "    \n",
    "    targets = np.zeros((batch_size,))\n",
    "    targets[batch_size//2:] = 1\n",
    "    \n",
    "    categories = np.random.choice(classes, size=(batch_size,), replace=False)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        \n",
    "        idx1 = np.random.randint(0, examples)\n",
    "        idx2 = np.random.randint(0, examples)\n",
    "        \n",
    "        if targets[i] == 0:\n",
    "            category_2 = category\n",
    "        else:\n",
    "            category_2 = (category + np.random.randint(1, classes)) % classes\n",
    "            \n",
    "        pairs[0][i,:,:,:] = dataset[category, idx1].reshape(w, h, 1)\n",
    "        pairs[1][i,:,:,:] = dataset[category_2, idx2].reshape(w, h, 1)\n",
    "    \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_val_batch(dataset, N):\n",
    "    \n",
    "    val_class, val_example, w, h = dataset.shape\n",
    "    \n",
    "    categories = np.random.choice(val_class, size=(N,), replace=False)\n",
    "    true_category = categories[0]\n",
    "    \n",
    "    indices = np.random.randint(0, val_example, size=(N,))\n",
    "    example1, example2 = np.random.choice(val_example, replace=False, size=(2,))\n",
    "    \n",
    "    valid_image = np.asarray([dataset[true_category, example1,:,:]]*N).reshape(N, w, h, 1)\n",
    "    \n",
    "    support_set = dataset[categories, indices,:,:]\n",
    "    \n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    support_set[0,:,:] = dataset[true_category, example2]\n",
    "    support_set = support_set.reshape(N, w, h, 1)\n",
    "    \n",
    "    pairs = [valid_image, support_set]\n",
    "    \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is not necessary but useful to ensure proper memory usage if you are using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model Architecture\n",
    "\n",
    "The code below outlines the convolutional neural network architecture. There is a shared base network in which images are inputted and ultimately output a vector in which images are classified as the same or different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def siamese_net():\n",
    "    \n",
    "    input_image = Input(shape=(105,105,1))\n",
    "    \n",
    "    siamese = Conv2D(64, kernel_size= (10,10), activation='relu', kernel_regularizer=l2(2e-4))(input_image)\n",
    "    siamese = MaxPooling2D()(siamese)\n",
    "    \n",
    "    siamese = Conv2D(128, kernel_size= (7,7), activation='relu', kernel_regularizer=l2(2e-4))(siamese)\n",
    "    siamese = MaxPooling2D()(siamese)\n",
    "    \n",
    "    siamese = Conv2D(128, kernel_size= (4,4), activation='relu', kernel_regularizer=l2(2e-4))(siamese)\n",
    "    siamese = MaxPooling2D()(siamese)\n",
    "    \n",
    "    siamese = Conv2D(256, kernel_size= (4,4), activation='relu', kernel_regularizer=l2(2e-4))(siamese)\n",
    "    \n",
    "    siamese = Flatten()(siamese)\n",
    "    output = Dense(4096, activation='sigmoid', kernel_regularizer=l2(2e-4))(siamese)\n",
    "    \n",
    "    siamese_model = Model(input_image, output)\n",
    "    \n",
    "    input_1 = Input(shape=(105,105,1))\n",
    "    input_2 = Input(shape=(105,105,1))\n",
    "    \n",
    "    output_1 = siamese_model(input_1)\n",
    "    output_2 = siamese_model(input_2)\n",
    "    \n",
    "    l1_distance_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    l1_distance = l1_distance_layer([output_1, output_2])\n",
    "    \n",
    "    prediction = Dense(1, activation='sigmoid')(l1_distance)\n",
    "    \n",
    "    siameseModel = Model(inputs=[input_1, input_2], outputs=prediction)\n",
    "    \n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    siameseModel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return siameseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4096)         38947648    input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 4096)         0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            4097        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 38,951,745\n",
      "Trainable params: 38,951,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38951745"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = siamese_net()\n",
    "model.summary()\n",
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy is 0.86\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "TRAIN_BATCH = 12000\n",
    "N_WAY = 20\n",
    "TEST_COUNT = 200\n",
    "validate_batch = 100\n",
    "best_val_acc = 0.0\n",
    "file_path = \"best_weights.hdf5\"\n",
    "\n",
    "for i in range(TRAIN_BATCH):\n",
    "    pairs, targets = create_batch(xTrain, batch_size)\n",
    "    targets = [[t] for t in targets]\n",
    "    loss = model.train_on_batch(pairs, targets)\n",
    "    if i % validate_batch == 0:\n",
    "        correct_tested = 0\n",
    "        for i in range(TEST_COUNT):\n",
    "            pairs, targets = create_val_batch(xValid, N_WAY)\n",
    "            pred = model.predict_on_batch(pairs)\n",
    "            \n",
    "            maxindex = np.argmin(pred)\n",
    "            \n",
    "            if maxindex == 0:\n",
    "                correct_tested += 1\n",
    "                \n",
    "        val_acc = correct_tested / TEST_COUNT\n",
    "        ## print(val_acc) COMMENTED OUT - UNCOMMENT IF YOU WANT TO TRACK VAL ACC\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            model.save_weights(file_path)\n",
    "\n",
    "print(\"Best accuracy is \" + str(best_val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Loss \n",
    "\n",
    "Performance is satisfactory on the above model but improvements can be made by using a different loss function. The previous model calculated the L1 distance and utilized binary cross-entropy to decrease the loss. The previous network worked to classify different images. Instead by using a contrastive loss function, the network can learn to maximize the ability to differentiate between images. \n",
    "\n",
    "Some notes for hyperparameters for the below code. Inclusion of epsilon in the euclidean distance function below is likely necessary as training often stalled out in the beginning otherwise. Similarly, the acc function which replaces the default accuracy function serves to help eliminate issues with very small values. Finally. the margin value in contrastive loss can be tuned to a different value but avoid making the margin value too low. \n",
    "\n",
    "For the network itself, I had to make sure to avoid using too much L2 regularization in the convolutional layers as the network would fail to train at all if there was too much regularization. If you encounter problems with the model failing to start training at the beginning, consider removing regularization hyperparameters. Additionally, I often encountered gradient explosion difficulties if I did not include kernel initalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "\n",
    "def euclidean_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    y_true = -1 * y_true + 1\n",
    "    return K.mean((1-y_true) * K.square(y_pred) + y_true *  K.square(K.maximum(margin - y_pred, 0.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def acc(y_true, y_pred):\n",
    "    ones = K.ones_like(y_pred)\n",
    "    return K.mean(K.equal(y_true, ones - K.clip(K.round(y_pred), 0, 1)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def W_init(shape, name=None):\n",
    "    values = np.random.normal(loc=0, scale=1e-2, size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "\n",
    "def contrastive_net():\n",
    "    \n",
    "    input_image = Input(shape=(105,105,1))\n",
    "    \n",
    "    siamese = Conv2D(64, kernel_size= (10,10), activation='relu', kernel_initializer=W_init, kernel_regularizer=l2(2e-4))(input_image)\n",
    "    siamese = MaxPooling2D()(siamese)\n",
    "    \n",
    "    siamese = Conv2D(128, kernel_size= (7,7), activation='relu', kernel_initializer=W_init, kernel_regularizer=l2(2e-4))(siamese)\n",
    "    siamese = MaxPooling2D()(siamese)\n",
    "    \n",
    "    siamese = Conv2D(128, kernel_size= (4,4), activation='relu', kernel_initializer=W_init, kernel_regularizer=l2(2e-4))(siamese)\n",
    "    siamese = MaxPooling2D()(siamese)\n",
    "    \n",
    "    siamese = Conv2D(256, kernel_size= (4,4), activation='relu', kernel_initializer=W_init, kernel_regularizer=l2(2e-4))(siamese)\n",
    "    \n",
    "    siamese = Flatten()(siamese)\n",
    "    output = Dense(4096, kernel_initializer=W_init, activation='sigmoid')(siamese)\n",
    "    \n",
    "    siamese_model = Model(input_image, output)\n",
    "    \n",
    "    input_1 = Input(shape=(105,105,1))\n",
    "    input_2 = Input(shape=(105,105,1))\n",
    "    \n",
    "    output_1 = siamese_model(input_1)\n",
    "    output_2 = siamese_model(input_2)\n",
    "    \n",
    "    distance = Lambda(euclidean_distance, output_shape=euclidean_dist_output_shape)([output_1, output_2])\n",
    "    \n",
    "    prediction = Dense(1, activation='sigmoid')(distance)\n",
    "    \n",
    "    siameseModel = Model(inputs=[input_1, input_2], outputs=prediction)\n",
    "\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    siameseModel.compile(loss=contrastive_loss, optimizer=optimizer, metrics=[acc])\n",
    "    \n",
    "    return siameseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 4096)         38947648    input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           model_3[1][0]                    \n",
      "                                                                 model_3[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            2           lambda_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 38,947,650\n",
      "Trainable params: 38,947,650\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38947650"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = contrastive_net()\n",
    "model.summary()\n",
    "model.count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is similar to the previous iteration. However, take note that we are predicting on the output of the intermediate layer in this new model. We need the distance vector and not the final output prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy is 0.905\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "TRAIN_BATCH = 12000\n",
    "TEST_COUNT = 200\n",
    "validate_batch = 100\n",
    "best_val_acc = 0.0\n",
    "file_path = \"contrastive_best_weights.hdf5\"\n",
    "\n",
    "intermediate_layer = Model(inputs=model.input, outputs=model.get_layer(index=3).output)\n",
    "\n",
    "for i in range(TRAIN_BATCH):\n",
    "    pairs, targets = create_batch(xTrain, batch_size)\n",
    "    targets = [[t] for t in targets]\n",
    "    loss = model.train_on_batch(pairs, targets)\n",
    "    if i % validate_batch == 0:\n",
    "        correct_tested = 0\n",
    "        for i in range(TEST_COUNT):\n",
    "            pairs, targets = create_val_batch(xValid, N_WAY)\n",
    "            pred = intermediate_layer.predict_on_batch(pairs)\n",
    "            \n",
    "            maxindex = np.argmin(pred)\n",
    "            \n",
    "            if maxindex == 0:\n",
    "                correct_tested += 1\n",
    "                \n",
    "        val_acc = correct_tested / TEST_COUNT\n",
    "        if val_acc == 1.0:\n",
    "            print(\"Gradient Explosion\")\n",
    "            break\n",
    "        ##print(val_acc) COMMENTED OUT - UNCOMMENT IF YOU WANT TO TRACK VAL ACC\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            model.save_weights(file_path)\n",
    "\n",
    "print(\"Best accuracy is \" + str(best_val_acc))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:aind-cv]",
   "language": "python",
   "name": "conda-env-aind-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
